---
layout:       post
title:        "Deep Learning"
author:       "Jacky"
header-style: text
catalog:      true
tags:
    - 人工智能
    - 深度学习
---

> 最近深度学习大火，人工智能又掀起了一波新的本着活到老学到老的精神，也来了解下深度学习。以下是一个机器学习小白的深度学习笔记。

## 0x00 导语
随着人工智能，本着活到老学到老的精神，也来了解下深度学习。以下是一个机器学习小白的深度学习笔记。

## 0x01 概念
+ 人工智能(Artificial Intelligence) ————为机器赋予人的智能
+ 机器学习(Machine Learning) ———— 一种实现人工智能的方法
+ 深度学习(Deep Learning) ———— 一种实现机器学习的技术

## 0x01 单层神经网络
机器学习可以通过神经网络来实现。可以将深度学习简单理解为，就是使用深度方法应用在神经网络的机器学习方法。目前深度架构大部分时候就是指深度神经网络。
### 1.神经元
1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。  
神经元模型是一个包含输入，输出与计算功能的模型。
![](/post_images/2mpmodel.jpg)  
其中的Sgn为激活函数   
![](/post_images/2jihuohanshu.png)  

图中Sum求和圆圈和符号函数组成一个神经元。  
连接是神经元中最重要的东西。每一个连接上都有一个权重。  
一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。  
MP模型虽然简单，但是是神经网络大厦的地基。

### 2.单层神经网络(感知器)
1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字--“感知器”(Perceptron)。  
在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，便有了最简单的神经网路。
![](/post_images/3Perceptron.jpg)  

假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。
![](/post_images/3Perceptron2.jpg)  

感知器中的权值是通过训练得到的。所谓训练，就是根据已有的输入和输出，找出最合适的权重参数。  
利用感知器，可以很好的完成线性分类任务。  
下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。 
![](/post_images/4pxiaoguo.png)  

1969年Minsky出版了一本叫《Perceptron》的书，里面用详细的数学证明了感知器的弱点, 感知器只能做简单的线性分类任务。  

## 0x02 两层神经网络
Minsky说过单层神经网络无法解决异或问题；两层神经网络可以解决异或问题，且具有非常好的非线性分类效果，但两层神经网络的计算是一个问题，没有一个较好的解法。  
### 1. 两层神经网络(多层感知器)
1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。   
![](/post_images/5liangceng.jpg)  
图中输入与输出的节点数是可以根据需要修改的。  
同时为了让计算更合理，每一层都会增加一个常量的偏置单元。  
![](/post_images/5liangceng2.jpg)  

使用矩阵运算来表达整个计算公式的话如下:(其中W/a/z都为矩阵)  
g(W(1) * a(1) + b(1)) = a(2);   
g(W(2) * a(2) + b(2)) = z;

理论证明，两层神经网络可以无限逼近任意连续函数。也就是说，面对复杂的非线性分类任务，两层神经网络可以分类的很好。  
下面就是一个[例子](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)，红色的线与蓝色的线代表数据。而红色区域和蓝色区域代表由神经网络划开的区域，两者的分界线就是决策分界。  
可以看到，这个两层神经网络的决策分界是非常平滑的曲线，而且分类的很好。  
![](/post_images/5xiaoguo2.png)  

### 2. 反向传播算法(BP)
机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为yp，真实目标为y。那么，定义一个值loss，计算公式如下。  
loss = (yp - y)^2  
这个值称之为损失(loss)，我们的目标就是使对所有训练数据的损失和尽可能的小。   
此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于0的运算量很大，所以一般来说解决这个优化问题使用的是梯度下降算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。  
![](/post_images/6tiduxiajiang.png)  
在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用反向传播算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。  
反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀E代表着相对导数的意思。
![](/post_images/6tidubp.jpg)  

## 0x03 深度学习
+ 随着BP算法的提出，神经网络也重新得到人们的重视，开始发力于语音识别，图像识别，自动驾驶等多个领域。  
+ 但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。  
+ 90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。  
+ 2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词--“深度学习”。  
+ 很快，深度学习在语音识别领域暂露头角。接着，2012年，深度学习技术又在图像识别领域大展拳脚。Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率15%的好成绩，这个成绩比第二名高了近11个百分点，充分证明了多层神经网络识别效果的优越性。  
多层神经网络如下:   
![](/post_images/7duocheng.jpg)  

通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。这点也在ImageNet的多次大赛中得到了证实。从2012年起，每年获得ImageNet冠军的深度神经网络的层数逐年增加，2015年最好的方法GoogleNet是一个多达22层的神经网络。  
在最近的ImageNet大赛上，目前拿到好成绩的MSRA团队的方法使用的更是一个深达152层的网络！  
目前，深度神经网络在人工智能界占据统治地位。但凡有关人工智能的产业报道，必然离不开深度学习。神经网络界当下的四位引领者除了前文所说的Ng，Hinton以外，还有CNN的发明人Yann Lecun，以及《Deep Learning》的作者Bengio。   
## 0x04 回顾
从单层神经网络（感知器）开始，到包含一个隐藏层的两层神经网络，再到多层的深度神经网络，一共有三次兴起过程。  

![](/post_images/8huigu1.jpg)  


下图说明了，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。  
![](/post_images/8huigu2.jpg)  

一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：

+ 更强的计算性能
+ 更多的数据 
+ 以及更好的训练方法。

## 0x05 其他
### 1.量子计算  
随着深度学习需要的计算量越来越大，目前的计算水平无法满足深度学习的要求，量子计算的发展，可能会一定程度决定机器学习的发展。

[KM:量子计算破晓，未来世界加速](http://km.oa.com/group/11800/articles/show/310998?kmref=search&from_page=1&no=6)

### 2.人工智能
虽然现在人工智能非常火热，但是距离真正的人工智能还有很大的距离。就拿计算机视觉方向来说，面对稍微复杂一些的场景，以及易于混淆的图像，计算机就可能难以识别。个人认为，目前人工智能全面替代人还有很长的路要走。
